{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adas754/generative-AI_class/blob/main/FAISS_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFxaGn_NlafU",
        "outputId": "7925dc50-641e-45b3-bfa6-3db34cd58775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=f5a364fe08d3c1914314919b4fa3dfbf1b979b124470cf0be372677a246a9c00\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoaJX0AKmMW8",
        "outputId": "3954ba1d-540e-4f9f-b84f-784d82dbcc7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "z6zRSgwInE49"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "rLxT4VCVnE3N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "0E0ZzBUXnE0z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dg_k6RhoN3i",
        "outputId": "8a6df882-c604-4658-8be6-7c30d1ec33da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351411017\\nReal-T ime Object Detection Using YOLO: A Review\\nPreprint  · May 2021\\nDOI: 10.13140/RG.2.2.24367.66723\\nCITATIONS\\n17READS\\n16,315\\n2 author s:\\nUpulie Handalag e\\nUniv ersität des Saarlandes\\n5 PUBLICA TIONS \\xa0\\xa0\\xa020 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nLakshini K uganandamurthy\\nSri Lank a Instit ute of Inf ormation T echnolog y\\n3 PUBLICA TIONS \\xa0\\xa0\\xa016 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Upulie Handalag e on 08 May 2021.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 0}),\n",
              " Document(page_content=\"Real-Time Object Detection using YOLO: A review  \\nUpulie H.D.I  \\nIT18107074  \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nireshaupulie@gmail.com   \\nLakshini Kuganandamurthy  \\nIT17073592   \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nlakkuga@gmail.com  \\n \\nAbstract—With the availability of eno rmous amounts of data \\nand the need to computerize visual -based systems, research on \\nobject detection has been the focus for the past decade. This need \\nhas been accelerated with the increasing computational power \\nand Convolutional Neural Network (CNN) advan cements since \\n2012. With various CNN network architectures available, the \\nYou Only Look Once (YOLO) network is popular due to its \\nmany reasons, mainly its speed of identification applicable in \\nreal-time object identification. Followed by a general \\nintroduc tion of the background and CNN, this paper wishes to \\nreview the innovative, yet comparatively simple approach \\nYOLO takes at object detection.  \\nKeywords —YOLO, CNN, object detection, image classification  \\n \\nI. INTRODUCTION  \\nAlthough the human eye is capable of instantly and \\nprecisely identifying a given visual, including its content, \\nlocation, and visuals close by interacting with it, the human \\nmade, computer vision -enabled systems are relatively low in \\naccuracy and speed. Any advancements leading to \\nimprovement s in efficiency and performance in this field \\ncould pave paths to creating more intelligent systems, much \\nlike humans. These advancements, in turn, would ease human \\nlife through systems such as assistive technologies that allow \\nhumans to complete tasks wit h little to no conscious thought. \\nFor instance, driving a car equipped with a computer vision -\\nenabled assistive technology could predict and notify a driving \\ncrash prior to the incident, even if the driver is not conscious \\nof their actions. Therefore, real -time object detection has \\nbecome a highly required subject in continuing the automation \\nor replacement of human tasks. Computer vision and object \\ndetection are prominent field s under machine learning and are \\neventually expected to aid unlocking the potential general -\\nresponsive robotic systems.  \\nWith the current technological advancements, creating \\nopenness and attainability of data to and from everyone \\nconnected to it has become  an easy task. Most human lives \\nrevolved around mainstream personal computers (PCs), and \\nsmartphones have made this process even more accessible. \\nAlong with this process, the expansion of information and \\nimages available on the internet/cloud has become to  the point \\nof millions per day. Usage of computerized systems to utilize \\nthis information and make necessary recognitions and \\nprocesses is vital due to humans' impracticality performing the \\nsame iterative tasks. The initial step of most such processes \\nmay include recognizing a specific object or area on an image. \\nDue to the unpredictability of the availability, location, size, \\nor shape of an item in each image, the recognition process is \\ninconceivably hard to be performed through a traditional \\nprogrammed co mputer algorithm. Factors such as the \\ncomplexity of the foundation, light intensities too contribute \\nto this.  Different strategies have been proposed to solve the \\nproblem of object identification throughout the years. These \\ntechniques focus on the solutio n through multiple stages. \\nNamely, these core stages include recognition, classification, \\nlocalization, and object detection. Along with the \\ntechnological progression over the years, these techniques \\nhave been facing challenges such as output accuracy, res ource \\ncost, processing speed and complexity issues. With the \\ninvention of the first Convolutional Neural Network (CNN) \\nalgorithm in the 1990s inspired by the Neocognitron by Yann \\nLeCun et al. [1] and significant inventions like AlexNet [2], \\nwhich won the ImageNet Large Scale Visual Recognition \\nChallenge (ILSVRC) in 2012 (thus later referred to as \\nImageNe t) CNN algorithms have been capable of providing \\nsolutions for the object detection problem in various \\napproaches. With the purpose of improving accuracy and \\nspeed of recognition, optimization focused algorithms such as \\nVGGNet [3], GoogLeNet [4] and Deep Residual Learning \\n(ResNet) [5] have been invented over the years.  \\nAlthough these algorithms improved over time, window \\nselection or identifying multiple objects from a single image \\nwas still an issue. To bring solu tions to this issue, algorithms \\nwith region proposals, crop/warp features, SVM \\nclassifications and bounding box regression such as Regions \\nwith CNN (R -CNN) were introduced. Although R -CNN was \\ncomparatively high in accuracy with the previous inventions, \\nits high usage of space and time later led to  the invention of  \\nSpatial Pyramid Pooling Network (SPPNet) [6]. Despite \\nSPPNet's speed , to reduce the similar drawbacks it shared with \\nR-CNN; Fast R -CNN was introduced. Although Fast R -CNN \\ncould reach real -time speeds using very deep networks, it held \\na computational bottleneck. Later Faster R -CNN, an \\nalgorithm based on ResNet, was introduced. Due to Faster R -\\nCNN not yet capable of surpassing state of the art detection \\nsystems, YOLO was introduced. This paper reviews the \\ndominating real -time object detection algorithm You Only \\nLook Once (YOLO).  \\nConsisting of layers in the basic CNN architecture and \\nYOLO network s, each layer's characteristics and t he two \\nversions of YOLO; YOLO -V1 and YOLO -V2 would be \\nreviewed under this paper. The strengths and weaknesses of \\nYOLO would be exposed, finally being fo llowed by a \\nsummarized conclusion.  \\n \\nII. CONVOLUTIONAL NEURAL NETWORK (CNN)  \\nA Convolutional Neural Network (CNN) could be taken \\nas a subcategory under Deep Neural Networks specifically \\ninvented for image processing and object detection. CNN \\nalgorithms can be ut ilized without requiring an enormous \\namount of predefined substantial parameters for the provided \\nimage. This ease at training a model and the vast amount of \", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 1}),\n",
              " Document(page_content=\"information available through the internet has made CNN \\nalgorithms possible. The mechanism CNN alg orithms follow \\nto express and extract features of the input data is entirely \\nmathematical. This mechanism involves a weight sharing \\nprocess that recognizes and identifies information that holds \\nsimilar features. This process enables networks to analyze \\nhigh data dimensions to achieve the final output of excellent \\nclassification in the end. One of the apparent obstacles in \\nmoving forward with getting better results using CNN \\nmodels is the processing capabilities of available hardware \\nand the scope of paramet ers in datasets.  \\nThe invention of the CNN [7] in 1998 with LeNet and its \\nbloom in 2012 with AlexNet was at the error rate of 15.3% \\nfollowed by ZF -net. The inventions of  GoogLeNe  and \\nVGGNet has made the error rate lower over time. An \\nexceptional milestone in this timeline was when ResNet \\nsurpassed the error rate of 3.6%, which was lower than that \\nof the human eye (5.1%) in 2015, proving that deep learning \\nmodels could surpass human ca pabilities.  \\n \\nA. Structure of CNN  \\nA typical CNN is structured with multiple layers: an \\ninput layer, a convolutional layer, an active layer, a pooling \\nlayer, a fully connected layer and finally, an output layer. \\nSome types of CNN models might include other laye rs for \\ndifferent purposes too . Figure 1 s hows the basic structure of \\na CNN architecture.  \\n \\n \\nFigure 1: The typical CNN structure with seven layers  \\n \\nSource:https://www.researchgate.net/publication/340102110_Hier\\narchical_Multi -View_Se mi-Supervised_Learning_for_Very_High -\\nResolution_Remote_Sensing_Image_Classification  \\n \\nThis multi -layered architecture is diverse in layers and \\nuses forward pass and error backpropagation calculations to \\nachieve the target's proficiency. Training this architecture to \\nbecome a model is a directed procedure that requires a \\ncollection of imager y data and their labels. Eventually, at the \\nend of the training process, the most suitable weights would \\nbe calculated to be used at the testing phase. These layers, as \\nmentioned above, could be further explained as follows.  \\n \\n \\n1) Input Layer  \\nThe input layer is used to initialize the input image data \\nand make all the available dimensions zero -centered. This \\nlayer is also responsible for normalizing the scale of all input \\ndata to a range within 0 and 1, which would help in \\naccelerating the speed of converging. This normalization is \\nalso helpful in reducing redundancy by whitening the data. \\nPrincipal Component Analysis (PCA) is done to degrade and \\ndecorate the available dimensions of the extracted data while \\nfocusing on key dimensions. [8] \\n \\n2) Convolutional Layer  As the layer, which is why CNN received its name, the \\nconvolutional layer is the most critical layer in a CNN \\nstructure.  Comprised of multiple element maps and many \\nneurons inside them, each of these neurons is created to \\nuntangle nearby qualities of various positions in the previous \\nlayer [9]. Many nearby associations and many mutual \\nattributes use a filter called CONV kernel, which slides on the \\noriginal image inputted to it. The CONV kernel calculates the \\nimage's component portrayal by multiplying and adding the \\nvalues of each pixel of the  local correlated data within it \\nbefore being added to the convolutional result. This so -called \\nrule of convolution enables the features of the image to be \\nextracted using the CONV kernel. The reason for filtering the \\nvarious parts of an image with the sam e CONV kernel is that \\nthis refers to shared weights. This usage of shared weights \\nenables neutral cells with the same features to be recognized \\nand classified into the same object type. Parameters such as \\nkernel size, depth, stride, zero -padding, and filte r quantity can \\nbe inputted onto this.  \\n \\n3) Active Layer  \\nThe active layer is the layer used to solve the problem of \\nthe vanishing gradient due to underfitting. This underfitting, \\nnonlinear problem is caused by the previous convolutional \\nlayer. One of the activ e layer functions such as Sigmoid, \\nTanh, the rectified Linear Unit (ReLu), the exponential \\nLinear Unit (ELU), Leaky ELU, or Maxout could be used in \\nsolving underfitting, following their usage [10]. Considering \\nthe converging speed, ReLu function has been the most \\npopula r although Sigmoid and Tanh functions are still \\ncommonly used due to their simplicity and efficiency.  \\n \\n4) Pooling Layer  \\nThe pooling layer's job is to efficiently reduce the \\ndimensions of the results sent from the convolutional layer. \\nThis is achieved by joini ng the neurons' outcome at one layer \\ninto a single neuron in the following layer, thus diminishing \\nthe elements of the component maps and incrementing the \\nstrength of selected extractions. Pooling layers are usually \\nsituated between two convolutional layer s and can be \\ncategorized into three distinct types based on their width: \\ngeneral pooling, overlapping pooling and Spatial Pyramid \\nPooling (SPP). A pooling layer is called a general pooling \\nlayer when its width is mainly equal to its stride. General \\npooling 's activities include max pooling and normal pooling. \\nWhen the most extreme incentives from each neuron group \\nfrom the previous layer are utilized, it is called max pooling. \\nWhen it is done for the normal incentives, it is referred to as \\nnormal pooling. Ov erlapping pooling is when the width is \\nlonger than the stride. Therefore, abnormal state attributes \\nfrom the input layer can be extracted and acquired by \\nstructuring a few convolutional layers along with a final \\npooling layer.  \\n \\n5) Fully Connected Layer  \\nOften the last layer before the output layer, the fully \\nconnected layer transmits data to the output layer while being \\nthe completely associated layer amongst the CNN layers. By \\nutilizing each neuron in the past layer and interfacing them to \\neach neuron on its o wn, it simplifies and speeds up the data \\ncalculation process. It being a completely associated layer \\nsaves no spatial data and is constantly trailed by a yield layer.  \\n\", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 2}),\n",
              " Document(page_content=\" \\n6) Other Layers  \\nApart from the different layers used in structuring a CNN \\nmodel mentioned a bove, some CNN models need additional \\nlayers to achieve the expected output. Layers such as dropout \\nlayers, regression layers come under this. Dropout layers are \\noften used to solve overfitting by avoiding majorly subjective \\nweights by updating weights of the neural cell knot with a \\ncertain probability (which is decided by the stochastic \\npolicy). Whereas, regression layer is used to classify features \\nusing a method such as logistic regression (LR), Bayesian \\nLinear Regression (BLR) and Gaussian Processes for  \\nRegression (GPR). The output of a regression layer is the \\nprobabilities of all the possible object types.  \\n \\nIII. TYPES OF OBJECT DETECTION ALGORITHMS  \\nAlgorithms available for object detection can be divided \\ninto two categories: classification -based algorithms a nd \\nregression -based algorithms.  \\n \\n1) Classification based algorithms  \\nClassification based algorithms are implemented in two \\nstages. The initial stage is the selection of region that is of \\ninterest  (RoI)  in the image. Then these regions are classified \\nwith the use of a convolutional neural network. This approach \\nof performing one stage prior to the other can be slow due to \\nthe need to run the prediction algorithm s on each region \\nselected in the first stag e. Few common examples for this \\ntype of algorithms are the Retina Net, Region -based CNN \\n(RCNN), the Fast -RCNN, Faster R -CNN and Mask -RCNN \\n(which is known to be a state -of-art under regional -based \\nCNN algorithms).  \\n \\n2) Regression -based algorithms  \\nRegression -based algorithms are implemented so that \\ninstead of selecting and singling out regions of interest in an \\nimage, they predict classes and their relevant bounding boxes \\nfor the whole image in one run through the model. Since \\nframe detection is treated as a regr ession problem, a complex \\npipeline is not necessary for regression -based algorithms. \\nFamous examples of this type of algorithms are the Single \\nShot Multibox Detector (SSD) and YOLO algorithms. Due to \\nthe simultaneousness of the detection and its nature of high \\nspeed (achieved with a tradeoff with accuracy), these are \\ncommonly used for real -time object detection. The detection \\nand understanding of the more popular YOLO algorithms \\nrequire an initial establishment of what will be predicted \\nbefore the models ar e used. The prediction would result in a \\nbounding box (specifying the Object's location) along with a \\nclass that has the highest probability amongst the established \\nset of classes.  \\n \\nIV. YOU ONLY LOOK ONCE (YOLO)  ALGORITHM  \\nYOLO is a novel approach to detect mu ltiple objects \\npresent in an image in real -time while drawing bounding \\nboxes around them. It passes the image through the CNN \\nalgorithm only once to get the output, thus the name. Although \\ncomparatively similar to R -CNN, YOLO practically runs a lot \\nfaster than Faster R -CNN because of its simpler architecture. \\nUnlike Faster R -CNN, YOLO can classify and perform bounding box regression at the same time. With YOLO, the \\nclass label containing objects, their location can be predicted \\nin one glance. Entirely devia ting from the typical CNN \\npipeline, YOLO treats object detection as a regression \\nproblem by spatially separating bounding boxes and their \\nrelated class probabilities, which are predicted using a single \\nneural network. This process of performing both boundi ng \\nbox prediction and class probability calculations is a unified \\nnetwork architecture that YOLO initially introduced.  \\nYOLO algorithm extends GoogLeNet equations to be used \\nas their base forwarding transport computation, assumably the \\nreason behind the spe ed and accuracy of YOLO's real -time \\nobject detection. In comparison with R -CNN architectures, \\nunlike running a classifier on a potential bounding box, then \\nreevaluating probability scores, YOLO predicts bounding \\nboxes and class probability for those boundi ng boxes \\nsimultaneously. This optimizes the YOLO algorithm and is \\none of the significant reasons why YOLO is so fast and less \\nlikely to have errors to be utilizable for real -time object \\npredictions.  \\nYOLO's architecture is similar to a typical convolutional  \\nneural network inspired by the GoogLeNet model for image \\nclassification. The network's initial layer first extracts the \\nimage's features, and the fully connected layers predict the \\noutput probabilities and coordinates. With 24 convolutional \\nlayers, two fu lly connected layers, 1x1 reduction layers and \\n3x3 convolutional layers, the full YOLO network model \\ncreated [12].  \\n \\nA. Unified Detection of YOLO  \\nYOLO is introduced as a unified algorithm as separate \\ncomponents merge into a single neural network as the final \\npipeline. For each bounding box to be predicted parallelly, the \\nfeatures of the entire image are globally reasoned. YOLO is \\ndesigned in such a way that it does its own end -to-end training \\nin real -time while keeping high -level average precision. To \\nachieve unified detection, YOLO first separates the input \\nimage into a S X S size grids. If the Object's center is being \\nplaced into the grid cell; the  grid cell tries object detection on \\nitself. Thus, every grid cell tries to estimate a bounding box \\nand their confidence scores across all classes trained to \\npredict. The predicted confidence scores will reflect how \\nconfident it is to provide each label an d bounding box to each \\nobject. Formally the confidence scores are defined as Pr \\n(Object) x IOUtruthpred . If an object has been found inside \\nthe cell, this confidence score will be equal to the intersection \\nover union (IOU) between the ground truth and the predicted \\nbox. If not, the confidence score would be equal to zero. The \\nunified detection outputs each confidence score to have five \\nparameters: w, y, w, h,  and confidence . The (x, y)  coordinates \\nrepresent the center of the box with respect to the grid cel l's \\nboundaries. As mentioned above, if the box's center does not \\nfall inside the grid cell, then the cell is not responsible for its \\nprediction. With each coordinate being normalized to be \\ncontained inside the range of 0 and 1, the estimated Object's \\nheigh t and width are calculated with respect to the entire \\nimage. According to Mauricio Menegaz in his article [11] the \\nprediction is of a few steps.  \", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 3}),\n",
              " Document(page_content=\" \\nFigure 2: Example of how to coordinate parameters are calcula ted \\nin a 448 X 448 image with S = 3  \\nSource: https://hackernoon.com/understanding -yolo-f5a74bbc7967  \\n \\n Figure 2 depicts how the x coordinate of (220 -149)/149 is \\nnormalized as 0.48. And y  coordinate of (190 -149)/149 is \\nnormalized as 0.28. The width (w) of 224 is calculated as \\n224/448 = 0.50 with respect to the entire image. And height \\n(h) of 143 is calculated as 143/448 = 0.32 with respect to the \\nentire image.  \\nThe confidence score predicts  the IOU among the \\nprediction box and the ground truth box along with these \\nparameters. This confidence score reflects the presence or \\nabsence of an object of any class inside the bounding box. \\nAlong with these calculations, every grid cell having an objec t \\nalso estimates the conditional class probabilities, given as Pr \\n(Class(i) | Object) . This probability is conditioned on the grid \\ncell containing one object. Therefore, if no object is present \\non the grid cell, the loss function will not penalize it for a  \\nwrong class prediction. Since the network will only predict \\none set of class probabilities per cell regardless of the number \\nof boxes ( B), the total number of class probabilities could be \\ntaken as S X S X C . It is said that at the time of testing when \\nconfidence scores for each box is individually calculated, the \\nconditional class probabilities and the individual box \\nconfidence predictions are multiplied as; Pr (Class (i) | \\nObject) X Pr (Object) X IOUtruthpred = Pr (Class(i)) X \\nIOUtruthpred . The confidence  scores for each box reflect the \\nclass's possibility being shown inside the box and how exactly \\nthe Object fits the estimated box.  \\n \\nB. How YOLO Algorithm works?  \\nYOLO algorithm is an algorithm based on regression. It \\npredicts class probabilities of the object  and bounding boxes \\nspecifying the object’s location, for the entire image. The \\nbounding boxes of the object are described as: bx, by, the x, \\ny coordinates represent the center of the box relative to the \\nbounds of the grid cell. The bw, bh as the width and  height \\nare predicted relative to the whole image and the value c is \\nrepresenting the class of the object. Y OLO  takes the image as \\ninput and divides it into S x S grids (3 x 3). Then, image \\nclassification and object localization techniques are applied \\nto each grid of the image and each grid is given a label. The \\nYOLO  algorithm then checks every grid for an object and \\nidentifies its label and bounding boxes. The label of a grid \\nthat does not have an object is indicated as zero. Every \\nlabelled grid is defined  as S.S having 8 values. The 8 values namely are pc, bx, by, bw, bh, c1, c2, c3. Pc shows if a \\nparticular grid has an object or not. If an object is available, \\nthe pc is assigned 1 else 0. bx, by, bh, bw are bounding box \\nparameters of a grid and are only d efined if a proper object is \\navailable in that grid. c1, c2, c3 are classes. If the object is a \\ncar, then the value of c1, c2, c3 are 0,1,0 respectively [11].  \\n \\n \\nFigure 3: Example image with 3x3 grids.  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\nIn the example grid, a proper object cannot be \\nidentified from the first grid. Therefore, pc value is 0 and \\nbounding box parameters need not be assigned as there is no \\ndefined object. Class probability cannot be identified as there \\nis no proper object (Fig ure 4). The 6th grid has a proper object \\nand therefore pc value is assigned 1 and bounding boxes for \\nthe object are bx, by, bw and bh. Since the object is a car, the \\nclasses for the grid are 0,1,0 (Figure 5) [11].  \\n \\n \\nFigure 4:Boundi ng box and class values for grid 1  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\n \\nFigure 5:Bounding box and class values for grid 6  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\n\", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 4}),\n",
              " Document(page_content=\"The matrix is defined as S  x S x 8, where S x S \\nrepresents the entire grid size, image gets divided into and 8 \\nindicates the total count of p c, bx, by, bw, bh, c1, c2, c3 \\nvalues. Bounding boxes differ for each grid depending on the \\nposition of objects in the relative grid. If more than two grids \\nhave the same object, then the grid cell that has the center of \\nthe object is used to detect that ob ject. For a precise \\nidentification of the object, two methods can be used ; 1. \\nIntersection over Union (IOU) 2. Non -Max Suppression [11]. \\nIn IOU, actual and estimated bounding box values are used \\nand the IOU of both values are computed using the following \\nformulae.  \\n \\n                             IOU = Intersection Area  \\n                                       ------------------------  \\n                                        Union Area  \\n \\nIt is better if the computed  IOU is greater than a \\nthreshold value (an assum ed value for increasing the accuracy \\nof the detected object.) 0.5  [11].   \\nIn Non -Max Suppression, the next method, high possibility \\nboxes are used and the boxes with high  IOU values are \\nsuppressed [11]. This process is followed many times until a \\nbox is considered as the bounding box for the object.  Each \\ngrid cell also predicts ‘c’ conditional class probabilities for \\nthe object in that grid. These probabilities are conditioned on \\nthe grid cell containing an object. Only one set of class \\nprobabilities is predicted for a grid cell, regardless of the \\nnumber of bounding boxes for tha t grid cell. [12] \\n \\n \\nFigure 6: Complete process of Object detection b y YOLO  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\nV. YOLO  VERSIONS  \\n• YOLOv1  \\nBase YOLO is also called YOLO Version 1[10]. It \\ndetects the object basing it as a regression problem. A single \\nconvolutional network predicts multiple bounding boxes and \\nclass probabilities for all the grid cells simultaneously. The \\ninput image is divided into S x S grids. If the center of a \\nproper object falls into a grid cell, that grid cell will be \\nconsidered in detecting that object. Each grid cell predicts N \\nbounding boxes and confidence scores for the boxes and c \\nclass probabilities. At test time, the  class probabilities and the \\nindividual box confidence predictions are multiplied resulting in class -specific confidence scores for each \\npredicted box. These scores encode both the probability of \\nthat class appearing in the box and how well the predicted \\nbox fits the object (Figure 6). These predictions are encoded \\nas an S × S × (N ∗ 5 + c) tensor. The width(bw) and the \\nheight(bh) are predicted relative to the whole image. And that \\nis why YOLOv1 uses Nx5 for calculating tensor. [10]  \\n \\n Pr(Class i |Object) ∗ Pr(Object) ∗ IOU pred = Pr(Class i \\n) ∗ IOU pred.  \\n \\nThese confidence scores reflect how confident the model \\nis in ensuring that the predicted box contains an object and \\nhow accurate the model thinks the box around the different \\nobjects is. Confidence score i s defined as:  \\n   Pr(Object) * IOUtruthpred  \\n \\nYOLOv1’s network has 24 convolutional layers as \\nopposed to YOLOv2, which has 19 layers [10]. For \\nevaluating YOLO model on the PASCAL VOC detection \\ndataset, these values are used: S=7, therefore a 7x7 grid. N=2, \\nnumber of bounding boxes. The PASCAL VOC dataset has \\n20 labelled classes so c=20. Therefore YOLOv1’s final \\nprediction is a 7x7x(5x2+20)=7x7x30 tensor. Here only 98 \\nbounding boxes per image is used [10], [12]  \\n \\n• YOLOv2  \\nYOLO Version 2 is an improved version of the existing \\nYOLO algorithm. The speed of detection performance \\nremains same while the mAP value increased compared to \\nYOLOv1’s Map value of 63.4. New multi -scale training \\nmethod can be used to run the YOLOv2 run at various sizes \\noffering improvements in a ccuracy and speed in prediction.  \\nYOLOv2 adds a list of significant solutions to increase mAP. \\nBatch Normalization preprocesses the input data. High \\nResolution Classifier from YOLOv1’s 224x224 to 448x448 \\nraises the mAP by 4%. Its neural network has 19 \\nconvo lutional layers compared to the YOLOv1 which has 24. \\nYOLOv2 adopts convolutional with anchor boxes and \\nincreases each grid cell’s resolution from YOLOv1’s 7x7 to \\n13x13. It also has only one bounding box for each grid cell. \\nFinally, YOLOv2 adds a pass -throu gh layer to get the \\nextracted features from the former layer and combine them \\nwith the original final output features, so that the ability of \\ndetecting the small object would be enhanced. In this mean, \\nYOLOv2 raises the mAP by 1% [10].  \\n \\nVI. STRENGTHS AND WEAKN ESSES OF YOLO  \\nYOLO is the state -of-art real -time object detection \\nalgorithm that surpasses the previous CNN detection speed \\nlimits while maintaining a good balance between speed and \\naccuracy. YOLOv2, the latest version of YOLO achieving a \\nmean Average Prec ision (mAP) rate of 76.8 at 67 Frames per \\nSecond (FPS) and 78.6 mAP rate at 7 6 FPS, outperforms \\nregional -based algorithms such as Faster R -CNN in both \\nspeed and accuracy. Another great strength in YOLO is its \\nglobal reasoning skills that encode the context ual information \\nabout the whole image rather than a specific region. Along \\nwith these global reasoning skills, the ability to predict false \\npositives in the background increases, improving the \\nalgorithm's reasoning skills as a whole. Lastly, with YOLO's \\n\", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 5}),\n",
              " Document(page_content=\"ability to learn basic representations of the labelled objects, it \\nhas outperformed other detection methods, including \\n(Deformable Part Model) DPM and R -CNN when \\ngeneralizing natural images amongst images like artwork. \\nDue to YOLO's generalizability and appl icability in new \\ndomains and unexpected outputs, it is considered one of the \\nbest object detection algorithms in the domain.  \\nAlthough YOLO has many unique strengths, it also \\nhas weaknesses. One of the notable weaknesses of YOLO is \\nits spatial constraints on bounding boxes. These spatial \\nconstraints are held due to each cell being able to predict only \\ntwo boxes and one class. It limits the number of predictable \\nobjects nearby to each other in groups (such as the \\nrecognition of a flock of birds, a basket of similar fruits). Due \\nto only being taught through input data, YOLO also has a \\nweakness in generalizing objects in unusual or new aspect \\nratios. However, this could be considered as more of a general \\nproblem in the object detection domain. Since the model o nly \\nuses relatively coarse features for prediction, the architecture \\nhas a few down sampling layers from the input images, which \\ncan be mentioned as a general weakness  of YOLO . Compared \\nwith an ideal algorithm, YOLO's loss function (which \\napproximates the detection performance) treats errors with \\nthe same loss despite its object boxes' size. This is not \\nadvantageous since a small error in a small box is not \\nequivalent to a small error in a large box, which has a more \\nsignificant effect on the Intersection o ver Union (IOU), \\ngiving out incorrect localizations in the end. Therefore, th ese \\ncould be taken as some of the areas YOLO algorithm could \\nimprove upon.   \\n \\nVII. CONCLUSION AND FUTURE SCOPE  \\nThis paper reviews the fundamental structure of CNN \\nalgorithms and an overview of YOLO's real -time object \\ndetection algorithm. CNN architecture models can remove \\nhighlights and discover objects in each given image. When \\nproperly used, CNN models can solve deformity \\nidentification, instructive/ learning application creation etc. \\nWhen in comparison with other CNN algorithms, YOLO has \\nmany advantages in practice. Being a unified object detection \\nmodel that is simple to construct and train in correspondence \\nwith its simple loss -function, YOLO can train the entire model \\nin parallel. The second major version of YOLO, YOLOv2, \\nprovides the state -of-art, best tradeoff between speed and \\naccuracy for object detection. YOLO is also better at \\ngeneralizing Object represen tation compared with other object \\ndetection models and can be recommended for real -time \\nobject detection as the state -of-art algorithm in object \\ndetection. With these marks, it is acknowledgeable that the \\nfield of object detection has an expanding, great f uture ahead.  \\nACKNOWLEDGEMENT  \\nThe authors would like to thank Dr Dharshana \\nKasthurirathna of the Faculty of Computing, SLIIT, to write \\nthis review paper as an assignment. The expertise of everyone \\nwho improved this study in numerous ways is also gratefully  \\nappreciated.  REFERENCES  \\n[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \\n“Gradient -Based Lerning Applied to Document \\nRecognition,” proc. IEEE , 1998, [Online]. Available: \\nhttp://ieeexplore.ieee.org/document/726791/#full -\\ntext-section.  \\n[2] T. F. Gonzalez, “Handbook of approximation \\nalgorithms and metaheuristics,” Handb. Approx. \\nAlgorithms Metaheuristics , pp. 1 –1432, 2007, doi: \\n10.1201/9781420010749.  \\n[3] K. Simonyan and A. Zisserman, “Very deep \\nconvolutional networks for large -scale image \\nrecognition,” 3rd Int. Conf. Learn. Represent. ICLR \\n2015 - Conf. Track Proc. , pp. 1 –14, 2015.  \\n[4] C. Szegedy et al. , “Going Deeper with \\nConvolutions,” 2015, doi: 10.1002/jctb.4820.  \\n[5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual \\nLearning for Image Recognition,” doi: \\n10.1002/chin.200650130.  \\n[6] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, \\n“What Makes for Effective Detection Proposals?,” \\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. \\n4, pp. 814 –830, 2016, doi: \\n10.1109/TPAMI.2015.2465908.  \\n[7] A. S. R. H. A. J. S. S. Carlsson, “CNN Feat ures off -\\nthe-shelf: an Astounding Baseline for Recognition,” \\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. \\nWork. , vol. 7389, pp. 806 –813, 2014, doi: \\n10.1117/12.827526.  \\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \\n“ImageNet Classification with Deep Convolutional \\nNeural Networks,” [Online]. Available: \\nhttps://www.cv -\\nfoundation.org//openaccess/content_cvpr_workshop\\ns_2014/W15/papers/Razavian_CNN_Features_Off -\\nthe-Shelf_2014_CVPR_paper.pdf.  \\n[9] T. Guo, J. Dong, H. Li, and Y. Gao, “Simple \\nconvolutional neu ral network on image \\nclassification,” 2017 IEEE 2nd Int. Conf. Big Data \\nAnal. ICBDA 2017 , pp. 721 –724, 2017, doi: \\n10.1109/ICBDA.2017.8078730.  \\n[10] J. Du, “Understanding of Object Detection Based on \\nCNN Family and YOLO,” J. Phys. Conf. Ser. , vol. \\n1004, no. 1, 2018, doi: 10.1088/1742 -\\n6596/1004/1/012029.  \\n[11] Mauricio Menegaz, “Understanding YOLO – Hacker \\nNoon,” Hackernoon . 2018.  \\n[12] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \\n“You only look once: Unified, real -time object \\ndetection,” 2016, doi: 10.11 09/CVPR.2016.91.  \\n \\nView publication stats\", metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 6})]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "_flwMPNBoRQu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "B5qAcZZ8oROd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8c0eI2EoQ47",
        "outputId": "300b6e0b-7319-4686-8938-ccd8a5c4ac35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAmiiUqGof3z",
        "outputId": "7cb9879c-0e75-4abf-c33f-2efb9c9dfd61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Real-Time Object Detection using YOLO: A review  \\nUpulie H.D.I  \\nIT18107074  \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nireshaupulie@gmail.com   \\nLakshini Kuganandamurthy  \\nIT17073592   \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nlakkuga@gmail.com  \\n \\nAbstract—With the availability of eno rmous amounts of data \\nand the need to computerize visual -based systems, research on \\nobject detection has been the focus for the past decade. This need', metadata={'source': 'pdfs/Real-TimeObjectDetectionusingYOLOAreview.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "O8p_NZWmoh3r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "ZstwNE_JpWaE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "qRQYVcPApZLs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore=FAISS.from_documents(text_chunks, embeddings)"
      ],
      "metadata": {
        "id": "9eWYZ5PpqDaE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is yolo?\""
      ],
      "metadata": {
        "id": "JeoqzXOOqK6n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "id": "1e7xVTiJqK4Q",
        "outputId": "343c4b4f-8049-49ac-97af-d45a00c1c723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is yolo?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "5YYNGtJEr_3A"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "agPjOGGyshYs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "giXzO7XpsR3w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()"
      ],
      "metadata": {
        "id": "wkOGn5_jsVpE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "Ec6wqwphsYxk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "2qv2VmTJsogq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is correaltion?\""
      ],
      "metadata": {
        "id": "h-4ngXzhsrb3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.run(query))"
      ],
      "metadata": {
        "id": "i_zqJPmvstmm",
        "outputId": "c4abced1-43e1-4b53-cbfb-3f3181fa3685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Correlation refers to the relationship or connection between different variables or data points. In the context of the given passage, it could refer to the correlation between nearby qualities and positions in the previous layer, or between the values of pixels in the local correlated data within the CONV kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhXoHHvJswHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}